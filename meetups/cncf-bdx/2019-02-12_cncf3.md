# Cloud Native Computing Foundation Bordeaux #3

## CNCF News
- New tech leaders have been appointed at CNCF in 2019 ;
- [etcd](https://coreos.com/etcd/) joined the CNCF incubator ;
- [CoreDNS](https://coredns.io/) has graduated!
- A [CVE](https://kubernetes.io/blog/2019/02/11/runc-and-cve-2019-5736/) has been discovered in RunC ;
- A translation initiative has started for the Kubernetes documentation: if you are interested, visit the [Kubernetes doc of your favorite language](https://kubernetes.io/fr/docs/home/) or fork the [website repository](https://github.com/kubernetes/website/tree/master/content)!

## Feedback on two years of production issues with Kubernetes
By [Denis Germain](https://twitter.com/zwindler), Cloud Engineer @ Lectra - [Personal Website \[FR\]](https://blog.zwindler.fr) - [Slides \[FR\]](https://blog.zwindler.fr/wp-content/uploads/2019/02/CNCF_meetup_Dans_Ton_Kube_REX_20190212.pdf)

Lectra is a company where 200+ developers are split across 22 teams including one for CI and a DevOps team. In 2015, it started its cloud & SaaS transition, which implies a progressive transition to Hign Availability by shutting down non-compatible infrastructure and migrating to micro-services.

The idea that struck right away: using Kubernetes, mostly with the Azure cloud. The Azure cloud allowed them to quickly deploy their infrastructure with [Kubespray](https://kubespray.io) or [Kubeadm](https://github.com/kubernetes/kubeadm), which are roughly similar. Today, their infrastructure is composed of seven clusters including five hosted in the Azure cloud and two on premise. They contain 50 nodes with 500 deployments and 1500 stateless pods. In two years, no major incident has held back production although some smaller ones were noticed. 

### The issues

[etcd](https://coreos.com/etcd/) is used by Kubernetes to describe cluster states. [AKS](https://docs.microsoft.com/en-us/azure/aks/), the managed Kubernetes solution from Azure uses an obsolete version of etcd - obsolete might be strong since the technology is very recent - which saturated the disks at Lectra's scale. Solution: manually migrate to a non-obsolete version of etcd.

As a reminder, in Kubernetes, the smaller unit is the Pod, which is defined in a ReplicaSet, which is defined in a Deployment. Kubernetes was not deploying pods optimally; some applications were hosted on the same node just because they were not demanding and the related nodes had more resources to spare at that moment. Solution: starting from Kubernetes 1.4, you can use [pod affinity and anti-affinity](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity) to specify how you prefer your pods to be distributed.

You might be tempted to allocate all your infrastructure on the cheapest region or the one you are doing the most business with. While this might look like a sensible decision, you might not be correctly protected against datacenter outages and other large scale issues. Serving your services on multiple Availability Zones is not easy nor cheap either: updates, maintenances and migrations get a lot more complex when distance comes at play.

When someone needs to plan a disruption for migration causes, it is advisable to have a [Pod Disruption Budget](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) set to control how your application will transition to its new version.

Deploying applications by hand can become tedious, especially when your JSON or YAML configs tend to get large. [Helm](https://helm.sh/) and Tiller can help with that by creating, deploying, updating and rollbacking configuration templates. Still, keep in mind that this solution is not Highly available and that storing the history of all deployments is very expensive. To reduce the impact of both issues at once, you could create one Helm-Tiller per namespace and limit its deployment history. This solution is not perfect, though, as it requires more resources.

In case your Kubernetes cluster is shared across many teams, it becomes hard to track the resource needs of every application. This situation can lead to the cluster resources getting hogged by an inefficient application. To avoid this, team developers can set Requests and Limits to indicate the minimum amount of CPU and Memory resources an application requires and the manimum it should be allowed to reach. What if those Limits and Requests are not present, then? First, it is recommended to set [Resource quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/) to evenly share resources. On top of that, a Kubernetes manager can set default limits for the applications running inside the namespace. This solution implies that each team will manage their own namespace.

When you want to expose an application to the real world, you can create [Services](https://kubernetes.io/docs/concepts/services-networking/service/) or rely on [Ingresses](https://kubernetes.io/docs/concepts/services-networking/ingress/). The latter is preferable as it allows more complete routing. NginX is the _de facto_ standard Ingress controller for Kubernetes, which is curious as it is not a CNCF-backed project, does not support hot-reload and which will detroy active connections and sockets when it needs to reload its vhost configuration. Kubernetes needs to reload its network configuration pretty often, especially on a cluster serving a large number of applications. Unfortunately, there is no perfect solution here; you can limit reloads, enable dynamic configuration at your own risk, have one Ingress controller per namespace or change the Ingress controller to [Envoy](https://www.envoyproxy.io/) or [Traefik](https://traefik.io/).

Always expose healthckeck routes in your application. Better: normalize thr routes that you will use for your endpoints like "/ping" for the readiness probe and "/health" for the liveness probe. Also, the healthcheck should not report the health of dependent services; if one of the dependent services fails, it might cause a fail in your current healthcheck and cause a domino effect in the best case scenario and a crash and restart loop in the worst case scenario. The worst case scenario is when two applications A and B healthcheck each other and one of the two fail for a reason, causing the other to misbehave and fail, etc.

## Chaos Engineering: Reliability by continuous learning
By [Sylvain Hellegouarch](https://twitter.com/lawouach), CTO / Co-founder @ ChaosIQ
