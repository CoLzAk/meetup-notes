# Cloud Native Computing Foundation Bordeaux #4

## CNCF time

### News
- CNCF Bordeaux is now an official CNCF group ðŸŽ‰;
- Kubernetes 1.14 is now generally available. Among the features: support for Windows nodes, updates in Kubectl and addition of Persistent Local Volumes. Go read the announcement [here](https://kubernetes.io/blog/2019/03/25/kubernetes-1-14-release-announcement/) ;
- The French translation initiative for the Kubernetes documentation is still ongoing. If you are interested, visit the [Kubernetes doc](https://kubernetes.io/fr/docs/home/) or fork the [website repository](https://github.com/kubernetes/website/tree/master/content)!

### Discovery of the session
If you want to learn a lot about the internals of Kubernetes, try [kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way) when you have some time to spare and some leftover credit on your favorite cloud provider.

## Kubinception
By [Kevin Georges](https://twitter.com/0xd33d33), OVH Metrics Lead & Kubernetes solution design manager @ OVH  
By Pierre Peronnet, Devops @ OVH  
By SÃ©bastien Jardin, Devops Engineer @ OVH, [Website](http://sebastienjardin.fr/)  
[Slides](https://www.slideshare.net/ovhcom/kubinception-using-kubernetes-to-run-kubernetes)

### Introduction

The OVH Managed Kubernetes platform has been released in beta during the OVH Summit, the 18th of October of 2018. The response has been fast; today the solution supports over 2000 active clusters, 10k pods on the administration clusters, 1500 CPUs and 22 Terabytes of RAM! The offer is managed because the Masters, a.k.a. Control plane, and some Node components, a.k.a. Compute, are taken in charge by OVH.

In their architecture, the Masters contain etcd, the OpenStack interface for Persistent Volumes, health checks, a load balancer, a route controller, the scheduler and API Server. This essential last component centralizes the connections between the Control plane and the Compute. Each node contains a kubelet and a kube-proxy, both managed by OVH as well.

In all of this, the critical components are etcd, API Server and kube-proxy. This project being very dependent of Kubernetes and having rewritten most of its core components, it is not recommended to try this at home.

### Kubernetes in Kubernetes?

The presented architecture actually contains itself! The Managed Kubernetes solution is a giant Kubernetes cluster which nodes are also Kubernetes! Only the client-facing nodes and etcd are managed separately from the system. This allows OVH to very quickly deploy new clusters.

Side-effect: internally, the communication is handled by kube-proxy and routed through the OVH Load Balancer. This allows OVH to use their dedicated network to their full potential and to provide a solid and performant backbone for the Managed Kubernetes solution. The interface to access the services is NodePort.

Admin nodes can communicate with API Server with TLS + SNI, meaning that the hostname appears in clear in the request. Client nodes can communicate through a TCP tunnel with API Server, using the kubelet and kube-proxy.

### Experience feedbacks: etcd

**etcd as a standalone Pod:** Simple to deploy but impossible to scale, non-persistent storage and risk of loss of quorum. The solution as been deemed impractical for the use case.

**etcd as an [Operator](https://coreos.com/operators/):** Easy to allocate and to manage its lifecycle but still non-persistent storage.

**StatefulSet / Persistent Volume:** Crash-resilient but no lifecycle management, terrible performance linked to network calls and costly.

**etcd on dedicated hardware:** Used in multi-tenant, cost-effective and dedicated resources. Logically separates clients using tokens and namespaces. This solution has been chosen as it is both the most performant and the easiest to manage.

### Networking

Th Nodes need to interact using the [Container Network Interface](https://github.com/containernetworking/cni). Such an infrastructure is very sensitive to latency and there are many implementations and many CVEs. OVH made the choice to rely on their [vRack](https://us.ovhcloud.com/products/networking/vrack-private-network) solution and route at Level 2 without using CNI!

Unfortunately, Level 2 routing implies that the heavy lifting must be handled by the infrastructure. Here, BREX (External Bridge) gives complete control over the network, which makes it a tempting solution. BREX requires ip_forward to be enabled. In addition, it rewrites the origin MAC address during SYN but not during SYN/ACK, which causes a broadcast. This can be problematic when a Node calls the API Server: when the number of nodes increases, one broadcast can essentially become an internal DDoS.

A solution that solves the broadcast issue for ARP requests resides in an ARP Kernel module which intrecepts connections and rewrites the MAC address exposed by the Pod to the one exposed by the Node.

### Kube-proxy

Kube-proxy is the component that handles communication with the Internet. It works in three modes:
- Userspace: Now deprecated, acted as an actual proxy which forwarded everything;
- IPTables: Loads IPTables rules in a blocking way. Very fast for small clusters and full of features but not very scalable;
- IPVS: Hashes Destination ports and IPs, which eventually reduces routing to locating the right hash. Handles connection loss and load balancing but not much more.

For the reload performance and the bandwidth limit, IPVS has been chosen. IPTables would handle 5k services while IPVS is able to scale to 25k before performance and bandwidth loss becomes noticeable.

### Questions & Answers session

Recommended read: [OVH Blog](https://www.ovh.com/fr/blog/)

- OVH maintains a Staging and a Production cluster and runs the CNCF CI  on them;
- OVH uses the OpenStack API for their cloud provider services with no reimplementation. That means that migrations are painless for OVH users;
- etcd is not replicated in multiple Availability Zones but is nonetheless replicated and backupped with the commits. Multi-AZ etcd would introduce latency issues and reduce the performance.

## Traefik: A scalable and HA edge router
By [Ã‰mile Vauge](https://twitter.com/emilevauge), Founder @ Containous, Creator of Traefik  
[Slides](https://containous.github.io/slides/cncf-bdx-2019/#/traefik)

### History

[...]

### Concepts

[...]

### Enterprise Edition

[...]

### The future

[...]
